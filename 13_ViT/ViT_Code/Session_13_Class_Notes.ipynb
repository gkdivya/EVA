{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 13 Class Notes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gkdivya/EVA/blob/main/13_ViT/ViT_Code/Session_13_Class_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s48QAdAyvFyQ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5mudTqyfQZS",
        "outputId": "dc8ef487-ec4c-42c3-a736-3cd0de3c9e90"
      },
      "source": [
        "import collections.abc\n",
        "\n",
        "def to_2tuple(x):\n",
        "  if isinstance(x, collections.abc.Iterable):\n",
        "    return x\n",
        "  return x, x\n",
        "\n",
        "\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "\n",
        "img_size = to_2tuple(img_size)\n",
        "patch_size = to_2tuple(patch_size)\n",
        "\n",
        "img_size, patch_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((224, 224), (16, 16))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBGoK3ibfri3",
        "outputId": "d697ae50-50a9-4a43-98bf-d74cefd66546"
      },
      "source": [
        "x = torch.rand((32, 3, 224, 224))\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 224, 224])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UO_BIp4fvHg",
        "outputId": "a9631f71-f0ad-4a9f-a6a5-0fec1e4ec416"
      },
      "source": [
        "proj = nn.Conv2d(3, 768, 16, 16)\n",
        "y = proj(x)\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 768, 14, 14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrLk0meIf0RV",
        "outputId": "49b7bdb9-fcc6-44c5-acb1-ef59742a2c8a"
      },
      "source": [
        "y.flatten(2).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 768, 196])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fNjdkY2f2B9",
        "outputId": "0af4c8d4-216d-4e00-a157-79d49ff3caf0"
      },
      "source": [
        "y.flatten(2).transpose(1, 2).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 196, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYwT6QOGf42o"
      },
      "source": [
        "class PatchEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Image to Patch Embedding.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_size=224, patch_size=16, num_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        image_size = to_2tuple(image_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.projection = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        batch_size, num_channels, height, width = pixel_values.shape\n",
        "        # FIXME look at relaxing size constraints\n",
        "        if height != self.image_size[0] or width != self.image_size[1]:\n",
        "            raise ValueError(\n",
        "                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size[0]}*{self.image_size[1]}).\"\n",
        "            )\n",
        "        x = self.projection(pixel_values).flatten(2).transpose(1, 2)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuguSPQBgcpG",
        "outputId": "6b14988b-5042-4d3f-cc39-c1534104a689"
      },
      "source": [
        "cls_token = nn.Parameter(torch.zeros(1, 1, 768))\n",
        "cls_token.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpF55k5BgifV",
        "outputId": "9fcb459c-d8a6-4c87-f077-e04f59148d79"
      },
      "source": [
        "cls_token.expand(32, -1, -1).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 1, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9SsEmtNgxOG",
        "outputId": "2fddfbd6-453e-4b36-f4cf-1fd99bfc59d6"
      },
      "source": [
        "position_embedding = nn.Parameter(torch.zeros(1, 14*14 + 1, 768))\n",
        "position_embedding.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKlEvVuXg7r6"
      },
      "source": [
        "class ViTEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Construct the CLS token, position and patch embeddings.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
        "        self.patch_embeddings = PatchEmbeddings(\n",
        "            image_size=config.image_size,\n",
        "            patch_size=config.patch_size,\n",
        "            num_channels=config.num_channels,\n",
        "            embed_dim=config.hidden_size,\n",
        "        )\n",
        "        num_patches = self.patch_embeddings.num_patches\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        batch_size = pixel_values.shape[0]\n",
        "        embeddings = self.patch_embeddings(pixel_values)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
        "        embeddings = embeddings + self.position_embeddings\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI7ry85Ti99F"
      },
      "source": [
        "class ViTConfig():\n",
        "  def __init__(\n",
        "        self,\n",
        "        hidden_size=768,\n",
        "        num_hidden_layers=12,\n",
        "        num_attention_heads=12,\n",
        "        intermediate_size=3072,\n",
        "        hidden_act=\"gelu\",\n",
        "        hidden_dropout_prob=0.0,\n",
        "        attention_probs_dropout_prob=0.0,\n",
        "        initializer_range=0.02,\n",
        "        layer_norm_eps=1e-12,\n",
        "        is_encoder_decoder=False,\n",
        "        image_size=224,\n",
        "        patch_size=16,\n",
        "        num_channels=3,\n",
        "        **kwargs\n",
        "    ):\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_act = hidden_act\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.initializer_range = initializer_range\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "\n",
        "configuration = ViTConfig()\n",
        "# You can read full configuration file here: https://github.com/huggingface/transformers/blob/master/src/transformers/models/vit/configuration_vit.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61e3JB35jKiF",
        "outputId": "c187991a-d44c-4072-b511-bf3605563ed4"
      },
      "source": [
        "vars(configuration)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_probs_dropout_prob': 0.0,\n",
              " 'hidden_act': 'gelu',\n",
              " 'hidden_dropout_prob': 0.0,\n",
              " 'hidden_size': 768,\n",
              " 'image_size': 224,\n",
              " 'initializer_range': 0.02,\n",
              " 'intermediate_size': 3072,\n",
              " 'layer_norm_eps': 1e-12,\n",
              " 'num_attention_heads': 12,\n",
              " 'num_channels': 3,\n",
              " 'num_hidden_layers': 12,\n",
              " 'patch_size': 16}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4P1KAlejNqT",
        "outputId": "bc921bb6-55b2-4f3d-d097-3275a969c78f"
      },
      "source": [
        "x = torch.rand(32, 3, 224, 224)\n",
        "vit_emb = ViTEmbeddings(configuration)\n",
        "vit_emb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTEmbeddings(\n",
              "  (patch_embeddings): PatchEmbeddings(\n",
              "    (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "  )\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34uW6dc6jXoR",
        "outputId": "9d11d3f9-7f5e-43ca-e44f-bce89f47b597"
      },
      "source": [
        "out = vit_emb(x)\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0ZBfCebjiRn",
        "outputId": "5dc77f7a-382b-4ff2-8ab8-417a35a42fdf"
      },
      "source": [
        "mat = nn.Linear(768, 12*64)\n",
        "mat = mat(out)\n",
        "mat.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvCdh6S8jqjm",
        "outputId": "cd2661a1-4830-48d2-df68-d7f2b13166d0"
      },
      "source": [
        "mat.size()[:-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmqIuOsDjwkj",
        "outputId": "97fd0546-9eb0-468c-fdb3-9df0a7c2d8cd"
      },
      "source": [
        "new_shape = mat.size()[:-1] + (12, 64)\n",
        "new_shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 12, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPbLdhszj3ox",
        "outputId": "51a13ae7-65a5-4235-c5e9-8c1f01e360c8"
      },
      "source": [
        "print(out.shape)\n",
        "out = out.view(*new_shape)\n",
        "print(out.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 197, 768])\n",
            "torch.Size([32, 197, 12, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLvFZeZHkAIt",
        "outputId": "a053bc9e-4661-4171-edc1-7ea0a5e5bec8"
      },
      "source": [
        "out = out.permute(0, 2, 1, 3)\n",
        "print(out.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 12, 197, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMYQKxRYkKly"
      },
      "source": [
        "out2 = out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "A_bAOl5CkN2c",
        "outputId": "2f912ecc-a0c4-4f42-d9bc-a5c64646dac8"
      },
      "source": [
        "torch.matmul(out, out2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-db20b78eb9ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjB3K53NkTF7",
        "outputId": "e35ab330-9d16-46bf-dc7a-94dd4096e4d4"
      },
      "source": [
        "out2.transpose(-1, -2).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 12, 64, 197])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTDws-jfkdcm",
        "outputId": "cda88c92-d963-4914-89b0-d9cdd064d667"
      },
      "source": [
        "attention_scores = torch.matmul(out, out2.transpose(-1, -2))\n",
        "attention_scores.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 12, 197, 197])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpKa1uKcks0F",
        "outputId": "a5d5ed44-a7f9-492a-9786-f1c1423d45e7"
      },
      "source": [
        "context_layer = torch.matmul(nn.Softmax(dim=-1)(attention_scores), out)\n",
        "context_layer.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 12, 197, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB5LQd1WlCK9",
        "outputId": "d3281625-fefc-43fc-8558-bc261ea6352e"
      },
      "source": [
        "context_layer = context_layer.permute(0, 2, 1, 3)\n",
        "context_layer.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 12, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNqXdMIJlWtB",
        "outputId": "f279d8c0-4c35-4a46-8ce1-93e33c919772"
      },
      "source": [
        "context_layer.size()[:-2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZDONXChlcYK",
        "outputId": "5fd133bb-ca75-4b17-aee6-051c3bb343b8"
      },
      "source": [
        "new_context_layer_shape = context_layer.size()[:-2] + (12*64,)\n",
        "new_context_layer_shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "SG0W463rlkHk",
        "outputId": "6f377197-c876-4593-cd63-9db2402aef82"
      },
      "source": [
        "print(context_layer.shape)\n",
        "context_layer.view(*new_context_layer_shape)\n",
        "print(context_layer.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 197, 12, 64])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-67dc3e434ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_context_layer_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ouv2ogPglohK",
        "outputId": "7917bcad-0304-4b34-ca20-b0342f9dca17"
      },
      "source": [
        "context_layer = torch.matmul(nn.Softmax(dim=-1)(attention_scores), out)\n",
        "print(context_layer.shape)\n",
        "context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "print(context_layer.shape)\n",
        "print(context_layer.size()[:-2])\n",
        "new_context_layer_shape = context_layer.size()[:-2] + (12*64,)\n",
        "print(new_context_layer_shape)\n",
        "print(context_layer.shape)\n",
        "context_layer.view(*new_context_layer_shape)\n",
        "print(context_layer.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 12, 197, 64])\n",
            "torch.Size([32, 197, 12, 64])\n",
            "torch.Size([32, 197])\n",
            "torch.Size([32, 197, 768])\n",
            "torch.Size([32, 197, 12, 64])\n",
            "torch.Size([32, 197, 12, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygHIsHPgmc1s",
        "outputId": "430c5a44-d080-4c92-cb9a-8dfb5fd0c4f0"
      },
      "source": [
        "x = torch.randn(3, 2)\n",
        "y = torch.transpose(x, 0, 1)\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2.4490, -0.4179],\n",
            "        [ 0.4150,  0.3402],\n",
            "        [-0.2985,  1.0702]])\n",
            "tensor([[ 2.4490,  0.4150, -0.2985],\n",
            "        [-0.4179,  0.3402,  1.0702]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mX9CQ-Zm7wr",
        "outputId": "9d6ab39a-9286-49b9-f5e5-a0e76e1c6942"
      },
      "source": [
        "x[0, 1] = 42\n",
        "print(x)\n",
        "print(y)\n",
        "print(x.is_contiguous())\n",
        "print(y.is_contiguous())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2.4490, 42.0000],\n",
            "        [ 0.4150,  0.3402],\n",
            "        [-0.2985,  1.0702]])\n",
            "tensor([[ 2.4490,  0.4150, -0.2985],\n",
            "        [42.0000,  0.3402,  1.0702]])\n",
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiqbHxCnnQy_"
      },
      "source": [
        "This is where the concept of contiguous comes in. In the example above, x is contiguous but y is not because its memory layout is different to that of a tensor of same shape made from scratch. Note that the word \"contiguous\" is a bit misleading because it's not that the content of the tensor is spread out around disconnected blocks of memory. Here bytes are still allocated in one block of memory but the order of the elements is different!\n",
        "\n",
        "When you call contiguous(), it actually makes a copy of the tensor such that the order of its elements in memory is the same as if it had been created from scratch with the same data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qF-c74ZnEoL"
      },
      "source": [
        "import math\n",
        "class ViTSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size {config.hidden_size,} is not a multiple of the number of attention \"\n",
        "                f\"heads {config.num_attention_heads}.\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, head_mask=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcEhf6z_p4ns",
        "outputId": "a18fb5bd-9759-4d05-a865-c78d0ebb61f8"
      },
      "source": [
        "vit_atn = ViTSelfAttention(configuration)\n",
        "vit_atn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTSelfAttention(\n",
              "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfrEsj0yp9Op",
        "outputId": "ab992a75-344e-4813-83c8-213aebc6f6cd"
      },
      "source": [
        "x = torch.rand((32, 3, 224, 224))\n",
        "vit_emb = ViTEmbeddings(configuration)\n",
        "vit_emb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTEmbeddings(\n",
              "  (patch_embeddings): PatchEmbeddings(\n",
              "    (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "  )\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CExnfoCgqFwQ",
        "outputId": "51a42e46-b5be-43c8-961d-10cb20709ea7"
      },
      "source": [
        "emb = vit_emb(x)\n",
        "emb.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFVGOigSqJq0"
      },
      "source": [
        "context_layer, attention_probs = vit_atn(emb, head_mask=None, output_attentions=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFZJ8tHAqVHO",
        "outputId": "14333032-391e-43dd-d6a7-00103589f66f"
      },
      "source": [
        "context_layer.shape, attention_probs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 197, 768]), torch.Size([32, 12, 197, 197]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNhx7_zAqe7Y"
      },
      "source": [
        "class ViTSelfOutput(nn.Module):\n",
        "  \"\"\"\n",
        "  This is just a Linear Layer Block\n",
        "  \"\"\"\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "  def forward(self, hidden_states, input_tensor):\n",
        "    hidden_states = self.dense(hidden_states)\n",
        "    hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "    return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMltqRtTqrss"
      },
      "source": [
        "class ViTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = ViTSelfAttention(config)\n",
        "        self.output = ViTSelfOutput(config)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        heads, index = find_pruneable_heads_and_indices(\n",
        "            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n",
        "        )\n",
        "\n",
        "        # Prune linear layers\n",
        "        self.attention.query = prune_linear_layer(self.attention.query, index)\n",
        "        self.attention.key = prune_linear_layer(self.attention.key, index)\n",
        "        self.attention.value = prune_linear_layer(self.attention.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "        # Update hyper params and store pruned heads\n",
        "        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n",
        "        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def forward(self, hidden_states, head_mask=None, output_attentions=False):\n",
        "        self_outputs = self.attention(hidden_states, head_mask, output_attentions)\n",
        "\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rWGBosTq5GB"
      },
      "source": [
        "class ViTIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = nn.functional.gelu(hidden_states)\n",
        "\n",
        "        return hidden_states\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzcYdPLqq_DE"
      },
      "source": [
        "class ViTOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        hidden_states = hidden_states + input_tensor\n",
        "\n",
        "        return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jdf7cyRrNMr"
      },
      "source": [
        "class ViTLayer(nn.Module):\n",
        "    \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = ViTAttention(config)\n",
        "        self.intermediate = ViTIntermediate(config)\n",
        "        self.output = ViTOutput(config)\n",
        "        self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states, head_mask=None, output_attentions=False):\n",
        "        self_attention_outputs = self.attention(\n",
        "            self.layernorm_before(hidden_states),  # in ViT, layernorm is applied before self-attention\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "\n",
        "        # first residual connection\n",
        "        hidden_states = attention_output + hidden_states\n",
        "\n",
        "        # in ViT, layernorm is also applied after self-attention\n",
        "        layer_output = self.layernorm_after(hidden_states)\n",
        "\n",
        "        layer_output = self.intermediate(layer_output)\n",
        "\n",
        "        # second residual connection is done here\n",
        "        layer_output = self.output(layer_output, hidden_states)\n",
        "\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output)\n",
        "        return layer_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkiJ0YP4ryGS"
      },
      "source": [
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([ViTLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        head_mask=None,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True,\n",
        "    ):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "\n",
        "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    layer_head_mask,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n",
        "        \n",
        "        return hidden_states,all_hidden_states,all_self_attentions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TGaDodCsRkQ"
      },
      "source": [
        "class ViTPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttyxLoM4sAVF"
      },
      "source": [
        "class ViTModel():\n",
        "    def __init__(self, config, add_pooling_layer=True):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = ViTEmbeddings(config)\n",
        "        self.encoder = ViTEncoder(config)\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.pooler = ViTPooler(config) if add_pooling_layer else None\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.patch_embeddings\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values=None,\n",
        "        head_mask=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        Returns:\n",
        "\n",
        "        Examples::\n",
        "\n",
        "            >>> from transformers import ViTFeatureExtractor, ViTModel\n",
        "            >>> from PIL import Image\n",
        "            >>> import requests\n",
        "\n",
        "            >>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "            >>> image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "            >>> feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "            >>> model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "            >>> inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "            >>> outputs = model(**inputs)\n",
        "            >>> last_hidden_states = outputs.last_hidden_state\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if pixel_values is None:\n",
        "            raise ValueError(\"You have to specify pixel_values\")\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings(pixel_values)\n",
        "\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        sequence_output = self.layernorm(sequence_output)\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return sequence_output,pooled_output,encoder_outputs.hidden_states,encoder_outputs.attentions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKdirDW_sbL5",
        "outputId": "7bd07af7-dbb3-4701-8ed3-fb114e05638a"
      },
      "source": [
        "out.shape, configuration"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 12, 197, 64]), <__main__.ViTConfig at 0x7f55a32162d0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HbqlvP0shN9"
      },
      "source": [
        "vit_enc = ViTEncoder(configuration)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPTUFkAEsjUZ",
        "outputId": "5124d67a-386a-4dac-bb7b-28bccea8db12"
      },
      "source": [
        "vit_enc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTEncoder(\n",
              "  (layer): ModuleList(\n",
              "    (0): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (1): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (2): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (3): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (4): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (5): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (6): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (7): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (8): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (9): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (10): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (11): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LITS8PADskSJ"
      },
      "source": [
        "input = torch.rand((32, 3, 224, 224))\n",
        "embeddings = ViTEmbeddings(configuration)\n",
        "encoder = ViTEncoder(configuration)\n",
        "layernorm = nn.LayerNorm(configuration.hidden_size, eps=0.000001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18uAzpcBst0a",
        "outputId": "2d2d361f-885f-4775-d1d4-8113a5950d33"
      },
      "source": [
        "embedding_output = embeddings(input)\n",
        "embedding_output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdeXM4vDsyL0"
      },
      "source": [
        "encoder_output = encoder(embedding_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWLcSaMdszk_",
        "outputId": "cfd80207-27e3-4e7e-83df-50a0d42f498b"
      },
      "source": [
        "type(encoder_output), len(encoder_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tuple, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2WWZJt1s23m",
        "outputId": "715a4688-fcaf-493d-fe79-acf32f6bff78"
      },
      "source": [
        "hidden_states, all_hidden_states, all_self_attentions = encoder_output\n",
        "\n",
        "hidden_states.shape, all_hidden_states, all_self_attentions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 197, 768]), None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ekf-F8as5AF",
        "outputId": "ec4a24af-306e-497c-a189-0837469b79de"
      },
      "source": [
        "sequence_output = encoder_output[0]\n",
        "sequence_output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II94R82Ys7ph",
        "outputId": "5f142ee5-13b1-49bd-a86a-8daf8bea5b38"
      },
      "source": [
        "sequence_output[:, 0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r31vA9tcs98U",
        "outputId": "fdde5a1f-e8e0-4576-9814-b3549ad7cd7f"
      },
      "source": [
        "sequence_output = encoder_output[0]\n",
        "layernorm = nn.LayerNorm(configuration.hidden_size, eps=0.00001)\n",
        "sequence_output = layernorm(sequence_output)\n",
        "# VitPooler\n",
        "dense = nn.Linear(configuration.hidden_size, configuration.hidden_size)\n",
        "activation = nn.Tanh()\n",
        "first_token_tensor = sequence_output[:, 0]\n",
        "pooled_output = dense(first_token_tensor)\n",
        "pooled_output = activation(pooled_output)\n",
        "pooled_output.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enqCeX5GtGbO",
        "outputId": "29c43909-3aaf-4dcb-b113-a8509e123b9d"
      },
      "source": [
        "classifier = nn.Linear(configuration.hidden_size, 100)\n",
        "logits = classifier(pooled_output)\n",
        "logits.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdsTtdfztKHj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}